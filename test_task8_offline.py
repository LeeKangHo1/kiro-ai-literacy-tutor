# test_task8_offline.py
# Task 8 ì˜¤í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (API í‚¤ ì—†ì´ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)

import os
import sys
import logging
from datetime import datetime
import unittest
from unittest.mock import Mock, patch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append('.')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TestTask8Offline(unittest.TestCase):
    """Task 8 ì˜¤í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.test_start_time = datetime.now()
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹œì‘: {self.test_start_time.strftime('%H:%M:%S')}")
    
    def test_error_handler_functionality(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("=== ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.error_handler import (
            handle_service_error, 
            get_all_service_status, 
            ServiceType, 
            ErrorSeverity,
            error_handler
        )
        
        # 1. ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ
        status = get_all_service_status()
        self.assertIsInstance(status, dict)
        self.assertIn('services', status)
        print(f"âœ… ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ ì„±ê³µ: {len(status['services'])}ê°œ ì„œë¹„ìŠ¤")
        
        # 2. í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ ìƒì„± ë° ì²˜ë¦¬
        error_result = handle_service_error(
            service_type=ServiceType.CHATGPT_API,
            error_code="test_error",
            error_message="í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜",
            context={"test": True},
            severity=ErrorSeverity.LOW
        )
        
        self.assertIsInstance(error_result, dict)
        print("âœ… ì˜¤ë¥˜ ì²˜ë¦¬ ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
        
        # 3. ì˜¤ë¥˜ í†µê³„ í™•ì¸
        from tools.external.error_handler import get_error_stats
        stats = get_error_stats(hours=1)
        self.assertIsInstance(stats, dict)
        self.assertIn('statistics', stats)
        print("âœ… ì˜¤ë¥˜ í†µê³„ ê¸°ëŠ¥ ì •ìƒ ì‘ë™")
    
    def test_api_monitor_functionality(self):
        """API ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("=== API ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.api_monitor import (
            api_monitor,
            log_api_call,
            check_rate_limit,
            rate_limiter
        )
        
        # 1. Rate limit í™•ì¸
        can_call = check_rate_limit()
        self.assertIsInstance(can_call, bool)
        print(f"âœ… Rate limit í™•ì¸: {can_call}")
        
        # 2. API í˜¸ì¶œ ë¡œê¹…
        log_api_call(
            endpoint="test/endpoint",
            success=True,
            response_time=0.5,
            token_usage={"total_tokens": 100},
            model="test-model"
        )
        
        # 3. ë©”íŠ¸ë¦­ ì¡°íšŒ
        metrics = api_monitor.get_current_metrics()
        self.assertIsInstance(metrics, dict)
        self.assertIn('total_calls', metrics)
        print(f"âœ… API ë©”íŠ¸ë¦­ ì¡°íšŒ: {metrics['total_calls']}íšŒ í˜¸ì¶œ")
        
        # 4. ìµœê·¼ í˜¸ì¶œ ê¸°ë¡ ì¡°íšŒ
        recent_calls = api_monitor.get_recent_calls(minutes=60)
        self.assertIsInstance(recent_calls, list)
        print(f"âœ… ìµœê·¼ í˜¸ì¶œ ê¸°ë¡: {len(recent_calls)}ê°œ")
    
    def test_prompt_quality_analyzer(self):
        """í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
        print("=== í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.chatgpt_tool import PromptQualityAnalyzer
        
        analyzer = PromptQualityAnalyzer()
        
        # 1. ì¢‹ì€ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        good_prompt = "AI ê¸°ìˆ ì˜ ë°œì „ì´ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. ê¸ì •ì  ì¸¡ë©´ê³¼ ë¶€ì •ì  ì¸¡ë©´ì„ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        good_analysis = analyzer.analyze_prompt_quality(good_prompt)
        
        self.assertIsInstance(good_analysis, dict)
        self.assertIn('overall_score', good_analysis)
        self.assertGreater(good_analysis['overall_score'], 0.5)
        print(f"âœ… ì¢‹ì€ í”„ë¡¬í”„íŠ¸ ë¶„ì„: ì ìˆ˜ {good_analysis['overall_score']:.2f}")
        
        # 2. ë‚˜ìœ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        bad_prompt = "AI"
        bad_analysis = analyzer.analyze_prompt_quality(bad_prompt)
        
        self.assertLess(bad_analysis['overall_score'], 0.5)
        self.assertGreater(len(bad_analysis['suggestions']), 0)
        print(f"âœ… ë‚˜ìœ í”„ë¡¬í”„íŠ¸ ë¶„ì„: ì ìˆ˜ {bad_analysis['overall_score']:.2f}, ì œì•ˆ {len(bad_analysis['suggestions'])}ê°œ")
    
    def test_web_search_dummy_mode(self):
        """ì›¹ ê²€ìƒ‰ ë”ë¯¸ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        print("=== ì›¹ ê²€ìƒ‰ ë”ë¯¸ ëª¨ë“œ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.web_search_tool import web_search_tool, search_web_for_answer
        
        # 1. ë”ë¯¸ ëª¨ë“œ í™•ì¸
        self.assertTrue(web_search_tool.dummy_mode)
        print("âœ… ë”ë¯¸ ëª¨ë“œ í™œì„±í™” í™•ì¸")
        
        # 2. ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ í…ŒìŠ¤íŠ¸
        results = search_web_for_answer("AIë€ ë¬´ì—‡ì¸ê°€ìš”?", max_results=3)
        
        self.assertIsInstance(results, list)
        self.assertGreater(len(results), 0)
        
        for result in results:
            self.assertIn('title', result)
            self.assertIn('snippet', result)
            self.assertIn('source', result)
        
        print(f"âœ… ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
    
    def test_fallback_strategies(self):
        """ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸"""
        print("=== ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.error_handler import (
            ChatGPTFallbackStrategy,
            ChromaDBFallbackStrategy,
            WebSearchFallbackStrategy,
            ServiceError,
            ServiceType,
            ErrorSeverity
        )
        from datetime import datetime
        
        # 1. ChatGPT ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸
        chatgpt_strategy = ChatGPTFallbackStrategy()
        chatgpt_error = ServiceError(
            service_type=ServiceType.CHATGPT_API,
            error_code="api_unavailable",
            error_message="API ì„œë¹„ìŠ¤ ì¤‘ë‹¨",
            severity=ErrorSeverity.HIGH,
            timestamp=datetime.now(),
            context={"request_type": "theory_explanation"}
        )
        
        self.assertTrue(chatgpt_strategy.is_applicable(chatgpt_error))
        chatgpt_result = chatgpt_strategy.execute({"request_type": "theory_explanation"})
        
        self.assertIsInstance(chatgpt_result, dict)
        self.assertIn('is_fallback', chatgpt_result)
        self.assertTrue(chatgpt_result['is_fallback'])
        print("âœ… ChatGPT ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        
        # 2. ChromaDB ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸
        chromadb_strategy = ChromaDBFallbackStrategy()
        chromadb_error = ServiceError(
            service_type=ServiceType.CHROMADB,
            error_code="connection_failed",
            error_message="DB ì—°ê²° ì‹¤íŒ¨",
            severity=ErrorSeverity.MEDIUM,
            timestamp=datetime.now(),
            context={"query": "AIë€ ë¬´ì—‡ì¸ê°€ìš”?"}
        )
        
        self.assertTrue(chromadb_strategy.is_applicable(chromadb_error))
        chromadb_result = chromadb_strategy.execute({"query": "AIë€ ë¬´ì—‡ì¸ê°€ìš”?"})
        
        self.assertIsInstance(chromadb_result, dict)
        self.assertIn('results', chromadb_result)
        self.assertGreater(len(chromadb_result['results']), 0)
        print("âœ… ChromaDB ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        
        # 3. ì›¹ ê²€ìƒ‰ ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸
        websearch_strategy = WebSearchFallbackStrategy()
        websearch_error = ServiceError(
            service_type=ServiceType.WEB_SEARCH,
            error_code="api_limit_exceeded",
            error_message="API í•œë„ ì´ˆê³¼",
            severity=ErrorSeverity.MEDIUM,
            timestamp=datetime.now(),
            context={"query": "ë¨¸ì‹ ëŸ¬ë‹"}
        )
        
        self.assertTrue(websearch_strategy.is_applicable(websearch_error))
        websearch_result = websearch_strategy.execute({"query": "ë¨¸ì‹ ëŸ¬ë‹"})
        
        self.assertIsInstance(websearch_result, dict)
        self.assertIn('is_fallback', websearch_result)
        print("âœ… ì›¹ ê²€ìƒ‰ ëŒ€ì²´ ì „ëµ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    def test_alert_system_functionality(self):
        """ì•Œë¦¼ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("=== ì•Œë¦¼ ì‹œìŠ¤í…œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.alert_system import AlertSystem, AlertRule, AlertChannel
        
        # ìƒˆë¡œìš´ ì•Œë¦¼ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        test_alert_system = AlertSystem()
        
        # 1. ê¸°ë³¸ ê·œì¹™ í™•ì¸
        active_rules = test_alert_system.get_active_rules()
        self.assertIsInstance(active_rules, list)
        self.assertGreater(len(active_rules), 0)
        print(f"âœ… ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™: {len(active_rules)}ê°œ")
        
        # 2. í…ŒìŠ¤íŠ¸ ê·œì¹™ ì¶”ê°€
        test_rule = AlertRule(
            name="test_rule",
            condition=lambda data: data.get("test") is True,
            message_template="í…ŒìŠ¤íŠ¸ ì•Œë¦¼: {message}",
            channels=[AlertChannel.LOG, AlertChannel.CONSOLE]
        )
        
        test_alert_system.add_rule(test_rule)
        updated_rules = test_alert_system.get_active_rules()
        self.assertEqual(len(updated_rules), len(active_rules) + 1)
        print("âœ… ì•Œë¦¼ ê·œì¹™ ì¶”ê°€ ì„±ê³µ")
        
        # 3. ì•Œë¦¼ ì´ë ¥ í™•ì¸
        alert_history = test_alert_system.get_alert_history()
        self.assertIsInstance(alert_history, list)
        print(f"âœ… ì•Œë¦¼ ì´ë ¥ ì¡°íšŒ: {len(alert_history)}ê°œ")
    
    def test_service_integration(self):
        """ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸"""
        print("=== ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸ ===")
        
        # 1. ëª¨ë“  ë„êµ¬ import í…ŒìŠ¤íŠ¸
        try:
            from tools.external import (
                search_knowledge_base,
                search_web_for_answer,
                chatgpt_api_tool,
                get_service_health_status,
                prompt_practice_tool
            )
            print("âœ… ëª¨ë“  ë„êµ¬ import ì„±ê³µ")
        except ImportError as e:
            self.fail(f"ë„êµ¬ import ì‹¤íŒ¨: {str(e)}")
        
        # 2. ì˜¤ë¥˜ ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸
        from tools.external.error_handler import get_all_service_status
        
        # ì—¬ëŸ¬ ì˜¤ë¥˜ ìƒì„±í•˜ì—¬ ì‹œìŠ¤í…œ ë°˜ì‘ í…ŒìŠ¤íŠ¸
        from tools.external.error_handler import handle_service_error, ServiceType, ErrorSeverity
        
        for i in range(3):
            handle_service_error(
                service_type=ServiceType.CHATGPT_API,
                error_code=f"test_error_{i}",
                error_message=f"í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜ {i}",
                context={"test_batch": True},
                severity=ErrorSeverity.LOW
            )
        
        status = get_all_service_status()
        self.assertGreater(status['total_errors_24h'], 0)
        print(f"âœ… í†µí•© ì˜¤ë¥˜ ì²˜ë¦¬: {status['total_errors_24h']}ê°œ ì˜¤ë¥˜ ê¸°ë¡")
    
    def test_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        print("=== ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ===")
        
        from tools.external.api_monitor import api_monitor
        import time
        
        # 1. ì—¬ëŸ¬ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        start_time = time.time()
        
        for i in range(5):
            from tools.external.api_monitor import log_api_call
            log_api_call(
                endpoint="test/performance",
                success=i % 4 != 0,  # 25% ì‹¤íŒ¨ìœ¨
                response_time=0.1 + (i * 0.05),
                token_usage={"total_tokens": 50 + i * 10},
                model="test-model"
            )
        
        # 2. ë©”íŠ¸ë¦­ í™•ì¸
        metrics = api_monitor.get_current_metrics()
        
        self.assertGreaterEqual(metrics['total_calls'], 5)
        self.assertGreater(metrics['success_rate'], 0.5)
        self.assertGreater(metrics['average_response_time'], 0)
        
        print(f"âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ - ì´ í˜¸ì¶œ: {metrics['total_calls']}, ì„±ê³µë¥ : {metrics['success_rate']:.1%}")
        
        # 3. ìµœê·¼ í˜¸ì¶œ ê¸°ë¡ í™•ì¸
        recent_calls = api_monitor.get_recent_calls(minutes=1)
        self.assertGreaterEqual(len(recent_calls), 5)
        print(f"âœ… ìµœê·¼ í˜¸ì¶œ ê¸°ë¡: {len(recent_calls)}ê°œ")

def run_comprehensive_test():
    """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Task 8 í¬ê´„ì ì¸ ì˜¤í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # í™˜ê²½ ì •ë³´ ì¶œë ¥
    print(f"\nPython ë²„ì „: {sys.version}")
    print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    suite = unittest.TestLoader().loadTestsFromTestCase(TestTask8Offline)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    total_tests = result.testsRun
    failures = len(result.failures)
    errors = len(result.errors)
    passed = total_tests - failures - errors
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {total_tests}")
    print(f"ì„±ê³µ: {passed}")
    print(f"ì‹¤íŒ¨: {failures}")
    print(f"ì˜¤ë¥˜: {errors}")
    print(f"ì„±ê³µë¥ : {passed/total_tests*100:.1f}%")
    
    if failures == 0 and errors == 0:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("\nâœ… Task 8 êµ¬í˜„ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤:")
        print("   - ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ âœ…")
        print("   - API ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ âœ…")
        print("   - í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ë¶„ì„ âœ…")
        print("   - ëŒ€ì²´ ì „ëµ ì‹œìŠ¤í…œ âœ…")
        print("   - ì•Œë¦¼ ì‹œìŠ¤í…œ âœ…")
        print("   - ì„œë¹„ìŠ¤ í†µí•© âœ…")
        print("   - ì„±ëŠ¥ ë©”íŠ¸ë¦­ âœ…")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        if failures:
            print("\nì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for test, traceback in result.failures:
                print(f"  - {test}: {traceback.split('AssertionError:')[-1].strip()}")
        if errors:
            print("\nì˜¤ë¥˜ê°€ ë°œìƒí•œ í…ŒìŠ¤íŠ¸:")
            for test, traceback in result.errors:
                print(f"  - {test}: {traceback.split('Exception:')[-1].strip()}")
    
    print(f"\ní…ŒìŠ¤íŠ¸ ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return passed == total_tests

if __name__ == "__main__":
    success = run_comprehensive_test()
    sys.exit(0 if success else 1)