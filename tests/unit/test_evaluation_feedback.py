# tests/unit/test_evaluation_feedback.py
# EvaluationFeedbackAgent ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸

import pytest
import sys
import os
from datetime import datetime
from unittest.mock import Mock, patch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from agents.evaluator.answer_evaluator import AnswerEvaluator, EvaluationResult
from agents.evaluator.feedback_generator import FeedbackGenerator
from agents.evaluator import EvaluationFeedbackAgent, create_evaluation_feedback_agent
from tools.evaluation.answer_eval_tool import (
    answer_evaluation_tool, 
    batch_evaluate_answers,
    calculate_learning_progress
)
from tools.evaluation.feedback_tool import (
    feedback_generation_tool,
    generate_quick_feedback,
    generate_adaptive_feedback
)


class TestAnswerEvaluator:
    """AnswerEvaluator í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        self.evaluator = AnswerEvaluator()
    
    def test_evaluate_multiple_choice_correct_answer(self):
        """ê°ê´€ì‹ ì •ë‹µ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "multiple_choice",
            "options": ["A", "B", "C", "D"],
            "correct_answer": 1,
            "difficulty": "medium",
            "explanation": "ì •ë‹µì€ Bì…ë‹ˆë‹¤."
        }
        
        result = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 1, hint_used=False, response_time=45
        )
        
        assert isinstance(result, EvaluationResult)
        assert result.is_correct == True
        assert result.score > 0
        assert result.understanding_level in ["low", "medium", "high"]
        assert len(result.strengths) > 0
        assert "question_type" in result.detailed_analysis
    
    def test_evaluate_multiple_choice_wrong_answer(self):
        """ê°ê´€ì‹ ì˜¤ë‹µ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "multiple_choice",
            "options": ["A", "B", "C", "D"],
            "correct_answer": 1,
            "difficulty": "medium",
            "explanation": "ì •ë‹µì€ Bì…ë‹ˆë‹¤."
        }
        
        result = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 0, hint_used=True, response_time=120
        )
        
        assert result.is_correct == False
        assert result.score == 0
        assert result.understanding_level == "low"
        assert len(result.weaknesses) > 0
    
    def test_evaluate_multiple_choice_with_hint_penalty(self):
        """íŒíŠ¸ ì‚¬ìš© ì‹œ ì ìˆ˜ í˜ë„í‹° í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "multiple_choice",
            "options": ["A", "B", "C", "D"],
            "correct_answer": 1,
            "difficulty": "medium"
        }
        
        # íŒíŠ¸ ë¯¸ì‚¬ìš©
        result_no_hint = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 1, hint_used=False
        )
        
        # íŒíŠ¸ ì‚¬ìš©
        result_with_hint = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 1, hint_used=True
        )
        
        assert result_no_hint.score > result_with_hint.score
        assert result_with_hint.detailed_analysis["hint_penalty"] < 0
    
    def test_evaluate_prompt_answer_good_quality(self):
        """ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "prompt_practice",
            "requirements": ["ì—­í•  ì •ì˜", "ì¹œê·¼í•œ í†¤", "êµ¬ì²´ì ì¸ ì§€ì‹œ"],
            "evaluation_criteria": ["ëª…í™•ì„±", "ì™„ì„±ë„", "êµ¬ì¡°ì„±"],
            "difficulty": "medium"
        }
        
        good_prompt = """
ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ê³ ê°ì„œë¹„ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. ë¨¼ì € ê³ ê°ì˜ ìƒí™©ì„ ê³µê°í•˜ë©° ì¸ì‚¬
2. ë¬¸ì œ ìƒí™©ì„ êµ¬ì²´ì ìœ¼ë¡œ íŒŒì•…
3. ë‹¨ê³„ë³„ í•´ê²° ë°©ì•ˆ ì œì‹œ
4. ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•œì§€ í™•ì¸

ì‘ë‹µì€ ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """.strip()
        
        result = self.evaluator.evaluate_prompt_answer(
            question_data, good_prompt, hint_used=False, response_time=180
        )
        
        assert result.is_correct == True  # 70ì  ì´ìƒ
        assert result.score >= 70
        assert result.understanding_level in ["medium", "high"]
        assert len(result.strengths) > 0
    
    def test_evaluate_prompt_answer_poor_quality(self):
        """ì €í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "prompt_practice",
            "requirements": ["ì—­í•  ì •ì˜", "ì¹œê·¼í•œ í†¤", "êµ¬ì²´ì ì¸ ì§€ì‹œ"],
            "evaluation_criteria": ["ëª…í™•ì„±", "ì™„ì„±ë„", "êµ¬ì¡°ì„±"],
            "difficulty": "medium"
        }
        
        poor_prompt = "ê³ ê°ì—ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result = self.evaluator.evaluate_prompt_answer(
            question_data, poor_prompt, hint_used=True, response_time=300
        )
        
        assert result.is_correct == False  # 70ì  ë¯¸ë§Œ
        assert result.score < 70
        assert result.understanding_level == "low"
        assert len(result.weaknesses) > 0
    
    def test_understanding_level_determination(self):
        """ì´í•´ë„ ìˆ˜ì¤€ ê²°ì • í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "multiple_choice",
            "options": ["A", "B", "C", "D"],
            "correct_answer": 0,
            "difficulty": "easy"
        }
        
        # ë†’ì€ ì´í•´ë„ (ì •ë‹µ, íŒíŠ¸ ë¯¸ì‚¬ìš©, ë¹ ë¥¸ ì‘ë‹µ)
        result = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 0, hint_used=False, response_time=20
        )
        assert result.understanding_level in ["medium", "high"]
        
        # ë‚®ì€ ì´í•´ë„ (ì˜¤ë‹µ)
        result = self.evaluator.evaluate_multiple_choice_answer(
            question_data, 1, hint_used=True, response_time=150
        )
        assert result.understanding_level == "low"


class TestFeedbackGenerator:
    """FeedbackGenerator í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        self.generator = FeedbackGenerator()
    
    def test_generate_comprehensive_feedback_correct_answer(self):
        """ì •ë‹µì— ëŒ€í•œ ì¢…í•© í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸"""
        evaluation_result = EvaluationResult(
            is_correct=True,
            score=85.0,
            understanding_level="high",
            strengths=["ì •ë‹µì„ ì •í™•íˆ ì„ íƒí–ˆìŠµë‹ˆë‹¤", "íŒíŠ¸ ì—†ì´ í•´ê²°í–ˆìŠµë‹ˆë‹¤"],
            weaknesses=[],
            detailed_analysis={"question_type": "multiple_choice", "difficulty": "medium"}
        )
        
        question_data = {
            "question_id": "test_q1",
            "question_type": "multiple_choice",
            "difficulty": "medium"
        }
        
        user_profile = {
            "user_level": "medium",
            "user_type": "beginner"
        }
        
        learning_context = {
            "current_chapter": 1,
            "recent_performance": []
        }
        
        feedback = self.generator.generate_comprehensive_feedback(
            evaluation_result, question_data, user_profile, learning_context
        )
        
        assert "feedback_id" in feedback
        assert "main_feedback" in feedback
        assert "encouragement" in feedback
        assert "improvement_tips" in feedback
        assert "next_steps" in feedback
        assert "ui_elements" in feedback
        assert "ğŸ‰" in feedback["main_feedback"] or "âœ…" in feedback["main_feedback"]
    
    def test_generate_comprehensive_feedback_wrong_answer(self):
        """ì˜¤ë‹µì— ëŒ€í•œ ì¢…í•© í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸"""
        evaluation_result = EvaluationResult(
            is_correct=False,
            score=0.0,
            understanding_level="low",
            strengths=[],
            weaknesses=["ì •ë‹µì„ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤", "íŒíŠ¸ê°€ í•„ìš”í–ˆìŠµë‹ˆë‹¤"],
            detailed_analysis={"question_type": "multiple_choice", "difficulty": "medium"}
        )
        
        question_data = {
            "question_id": "test_q1",
            "question_type": "multiple_choice",
            "difficulty": "medium",
            "user_answer": 0,
            "correct_answer": 1,
            "options": ["A", "B", "C", "D"],
            "explanation": "ì •ë‹µì€ Bì…ë‹ˆë‹¤."
        }
        
        user_profile = {
            "user_level": "medium",
            "user_type": "beginner"
        }
        
        learning_context = {
            "current_chapter": 1,
            "recent_performance": []
        }
        
        feedback = self.generator.generate_comprehensive_feedback(
            evaluation_result, question_data, user_profile, learning_context
        )
        
        assert "âŒ" in feedback["main_feedback"]
        assert len(feedback["improvement_tips"]) > 0
        assert feedback["next_steps"]["action"] == "review"
    
    def test_generate_prompt_feedback(self):
        """í”„ë¡¬í”„íŠ¸ ë¬¸ì œ í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸"""
        evaluation_result = EvaluationResult(
            is_correct=True,
            score=75.0,
            understanding_level="medium",
            strengths=["ìš”êµ¬ì‚¬í•­ì„ ì˜ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤"],
            weaknesses=["í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê°œì„  í•„ìš”"],
            detailed_analysis={
                "question_type": "prompt_practice",
                "requirements_score": 80,
                "quality_score": 70,
                "effectiveness_score": 75
            }
        )
        
        question_data = {
            "question_id": "test_prompt_1",
            "question_type": "prompt_practice"
        }
        
        user_profile = {"user_level": "medium", "user_type": "business"}
        learning_context = {"current_chapter": 3, "recent_performance": []}
        
        feedback = self.generator.generate_comprehensive_feedback(
            evaluation_result, question_data, user_profile, learning_context
        )
        
        assert "ğŸ‘" in feedback["main_feedback"] or "ğŸ“" in feedback["main_feedback"]
        assert "ìš”êµ¬ì‚¬í•­ ì¶©ì¡±ë„" in feedback["main_feedback"]
        assert "í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ" in feedback["main_feedback"]
    
    def test_personalized_message_generation(self):
        """ê°œì¸í™”ëœ ë©”ì‹œì§€ ìƒì„± í…ŒìŠ¤íŠ¸"""
        evaluation_result = EvaluationResult(
            is_correct=True,
            score=90.0,
            understanding_level="high",
            strengths=["ì™„ë²½í•œ ë‹µë³€"],
            weaknesses=[],
            detailed_analysis={}
        )
        
        # ì´ˆë³´ì í”„ë¡œí•„
        beginner_profile = {"user_type": "beginner", "user_level": "medium"}
        # ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œí•„
        business_profile = {"user_type": "business", "user_level": "high"}
        
        question_data = {"question_type": "multiple_choice"}
        learning_context = {"recent_performance": []}
        
        beginner_feedback = self.generator.generate_comprehensive_feedback(
            evaluation_result, question_data, beginner_profile, learning_context
        )
        
        business_feedback = self.generator.generate_comprehensive_feedback(
            evaluation_result, question_data, business_profile, learning_context
        )
        
        # ì‚¬ìš©ì ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì‹œì§€ê°€ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸
        assert beginner_feedback["personalized_message"] != business_feedback["personalized_message"]


class TestEvaluationFeedbackAgent:
    """EvaluationFeedbackAgent ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        self.agent = EvaluationFeedbackAgent()
    
    def test_agent_creation(self):
        """ì—ì´ì „íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        assert self.agent.agent_name == "EvaluationFeedbackAgent"
        assert hasattr(self.agent, 'answer_evaluator')
        assert hasattr(self.agent, 'feedback_generator')
    
    def test_execute_with_multiple_choice_answer(self):
        """ê°ê´€ì‹ ë‹µë³€ ì²˜ë¦¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        state = {
            "user_id": "test_user",
            "user_level": "medium",
            "user_type": "beginner",
            "current_chapter": 1,
            "user_answer": 1,
            "hint_used": False,
            "response_time": 45,
            "current_loop_conversations": [
                {
                    "agent": "QuizGenerator",
                    "quiz_data": {
                        "question_id": "test_q1",
                        "question_type": "multiple_choice",
                        "options": ["A", "B", "C", "D"],
                        "correct_answer": 1,
                        "explanation": "ì •ë‹µì€ Bì…ë‹ˆë‹¤."
                    }
                }
            ]
        }
        
        result_state = self.agent.execute(state)
        
        assert "system_message" in result_state
        assert "ui_elements" in result_state
        assert result_state["ui_mode"] == "feedback"
        assert result_state["current_stage"] == "feedback_review"
        assert "last_evaluation" in result_state
        assert "last_feedback" in result_state
        assert len(result_state["current_loop_conversations"]) > 1
    
    def test_execute_with_prompt_answer(self):
        """í”„ë¡¬í”„íŠ¸ ë‹µë³€ ì²˜ë¦¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        state = {
            "user_id": "test_user",
            "user_level": "medium",
            "user_type": "business",
            "current_chapter": 3,
            "user_answer": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ ê³ ê°ì„œë¹„ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤. ê³ ê°ì˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.",
            "hint_used": False,
            "current_loop_conversations": [
                {
                    "agent": "QuizGenerator",
                    "quiz_data": {
                        "question_id": "test_prompt_1",
                        "question_type": "prompt_practice",
                        "scenario": "ê³ ê° ì„œë¹„ìŠ¤",
                        "requirements": ["ì¹œê·¼í•œ í†¤", "êµ¬ì²´ì  í•´ê²°ì±…"]
                    }
                }
            ]
        }
        
        result_state = self.agent.execute(state)
        
        assert result_state["ui_mode"] == "feedback"
        assert "last_evaluation" in result_state
        assert "í”„ë¡¬í”„íŠ¸" in result_state["system_message"] or "ì ìˆ˜" in result_state["system_message"]
    
    def test_handle_chatgpt_test(self):
        """ChatGPT í…ŒìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        state = {
            "user_id": "test_user"
        }
        
        test_prompt = "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        
        result_state = self.agent.handle_chatgpt_test(state, test_prompt)
        
        assert "chatgpt_test_result" in result_state
        assert "system_message" in result_state
        assert "ChatGPT í…ŒìŠ¤íŠ¸ ê²°ê³¼" in result_state["system_message"]
    
    def test_execute_without_quiz_data(self):
        """í€´ì¦ˆ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        state = {
            "user_id": "test_user",
            "current_loop_conversations": []
        }
        
        result_state = self.agent.execute(state)
        
        assert "í‰ê°€í•  ë¬¸ì œë‚˜ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in result_state["system_message"]
        assert result_state["ui_mode"] == "chat"


class TestEvaluationTools:
    """í‰ê°€ ë„êµ¬ í…ŒìŠ¤íŠ¸"""
    
    def test_answer_evaluation_tool_multiple_choice(self):
        """ê°ê´€ì‹ ë‹µë³€ í‰ê°€ ë„êµ¬ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "multiple_choice",
            "options": ["A", "B", "C", "D"],
            "correct_answer": 1,
            "difficulty": "medium"
        }
        
        result = answer_evaluation_tool(
            question_data=question_data,
            user_answer=1,
            hint_used=False,
            response_time=45
        )
        
        assert result["success"] == True
        assert "evaluation_result" in result
        assert result["evaluation_result"]["is_correct"] == True
        assert result["evaluation_result"]["score"] > 0
    
    def test_answer_evaluation_tool_prompt_practice(self):
        """í”„ë¡¬í”„íŠ¸ ì‹¤ìŠµ ë‹µë³€ í‰ê°€ ë„êµ¬ í…ŒìŠ¤íŠ¸"""
        question_data = {
            "question_type": "prompt_practice",
            "requirements": ["ì—­í•  ì •ì˜", "ì¹œê·¼í•œ í†¤"]
        }
        
        prompt_answer = "ë‹¹ì‹ ì€ ì¹œê·¼í•œ ê³ ê°ì„œë¹„ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤. ê³ ê°ì„ ë„ì™€ì£¼ì„¸ìš”."
        
        result = answer_evaluation_tool(
            question_data=question_data,
            user_answer=prompt_answer,
            hint_used=False
        )
        
        assert result["success"] == True
        assert result["evaluation_result"]["question_type"] == "prompt_practice"
        assert "requirements_analysis" in result["evaluation_result"]["detailed_analysis"]
    
    def test_batch_evaluate_answers(self):
        """ì¼ê´„ ë‹µë³€ í‰ê°€ í…ŒìŠ¤íŠ¸"""
        questions_and_answers = [
            {
                "question_data": {
                    "question_type": "multiple_choice",
                    "options": ["A", "B", "C", "D"],
                    "correct_answer": 0
                },
                "user_answer": 0,
                "hint_used": False
            },
            {
                "question_data": {
                    "question_type": "multiple_choice",
                    "options": ["A", "B", "C", "D"],
                    "correct_answer": 1
                },
                "user_answer": 2,
                "hint_used": True
            }
        ]
        
        result = batch_evaluate_answers(questions_and_answers)
        
        assert result["success"] == True
        assert "batch_results" in result
        assert "statistics" in result
        assert len(result["batch_results"]) == 2
        assert result["statistics"]["total_questions"] == 2
        assert result["statistics"]["correct_count"] == 1
        assert result["statistics"]["accuracy_rate"] == 0.5
    
    def test_calculate_learning_progress(self):
        """í•™ìŠµ ì§„ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        evaluation_results = [
            {"is_correct": True, "score": 85, "understanding_level": "high"},
            {"is_correct": False, "score": 45, "understanding_level": "low"},
            {"is_correct": True, "score": 92, "understanding_level": "high"},
            {"is_correct": True, "score": 78, "understanding_level": "medium"}
        ]
        
        result = calculate_learning_progress("test_user", 1, evaluation_results)
        
        assert result["success"] == True
        assert "progress_data" in result
        progress = result["progress_data"]
        assert "completion_rate" in progress
        assert "average_score" in progress
        assert "accuracy_trend" in progress
        assert "understanding_trend" in progress


class TestFeedbackTools:
    """í”¼ë“œë°± ë„êµ¬ í…ŒìŠ¤íŠ¸"""
    
    def test_feedback_generation_tool(self):
        """í”¼ë“œë°± ìƒì„± ë„êµ¬ í…ŒìŠ¤íŠ¸"""
        evaluation_result = {
            "is_correct": True,
            "score": 85.0,
            "understanding_level": "high",
            "strengths": ["ì •ë‹µì„ ì •í™•íˆ ì„ íƒí–ˆìŠµë‹ˆë‹¤"],
            "weaknesses": [],
            "detailed_analysis": {"question_type": "multiple_choice"}
        }
        
        question_data = {
            "question_id": "test_q1",
            "question_type": "multiple_choice"
        }
        
        user_profile = {
            "user_level": "medium",
            "user_type": "beginner"
        }
        
        learning_context = {
            "current_chapter": 1,
            "recent_performance": []
        }
        
        result = feedback_generation_tool(
            evaluation_result, question_data, user_profile, learning_context
        )
        
        assert result["success"] == True
        assert "feedback_data" in result
        assert "main_feedback" in result["feedback_data"]
        assert "encouragement" in result["feedback_data"]
    
    def test_generate_quick_feedback(self):
        """ë¹ ë¥¸ í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸"""
        result = generate_quick_feedback(
            is_correct=True,
            score=90.0,
            question_type="multiple_choice",
            user_type="beginner"
        )
        
        assert result["success"] == True
        assert "feedback_data" in result
        assert result["feedback_data"]["is_correct"] == True
        assert "ğŸ‰" in result["feedback_data"]["main_feedback"]
    
    def test_generate_adaptive_feedback(self):
        """ì ì‘í˜• í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸"""
        evaluation_result = {
            "is_correct": True,
            "score": 85.0,
            "understanding_level": "high"
        }
        
        user_performance_history = [
            {"is_correct": True, "score": 80},
            {"is_correct": False, "score": 45},
            {"is_correct": True, "score": 90},
            {"is_correct": True, "score": 85}
        ]
        
        result = generate_adaptive_feedback(
            evaluation_result, user_performance_history, "medium"
        )
        
        assert result["success"] == True
        assert "feedback_data" in result
        assert "performance_analysis" in result["feedback_data"]
        assert "next_steps" in result["feedback_data"]


def test_create_evaluation_feedback_agent():
    """EvaluationFeedbackAgent ìƒì„± í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    agent = create_evaluation_feedback_agent()
    assert isinstance(agent, EvaluationFeedbackAgent)
    assert agent.agent_name == "EvaluationFeedbackAgent"


if __name__ == "__main__":
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ
    pytest.main([__file__, "-v"])