# tests/integration/test_quiz_evaluation_integration.py
# QuizGeneratorì™€ EvaluationFeedbackAgent í†µí•© í…ŒìŠ¤íŠ¸

import pytest
import sys
import os
from datetime import datetime
from unittest.mock import Mock, patch

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from agents.quiz import QuizGenerator
from agents.evaluator import EvaluationFeedbackAgent
from tools.content.quiz_tool import quiz_generation_tool
from tools.evaluation.answer_eval_tool import answer_evaluation_tool
from tools.evaluation.feedback_tool import feedback_generation_tool


class TestQuizEvaluationIntegration:
    """QuizGeneratorì™€ EvaluationFeedbackAgent í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰ ì „ ì„¤ì •"""
        self.quiz_agent = QuizGenerator()
        self.eval_agent = EvaluationFeedbackAgent()
    
    def test_complete_multiple_choice_flow(self):
        """ì™„ì „í•œ ê°ê´€ì‹ ë¬¸ì œ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # 1. í€´ì¦ˆ ìƒì„±
        initial_state = {
            "user_message": "ê°ê´€ì‹ ë¬¸ì œë¥¼ ë‚´ì£¼ì„¸ìš”",
            "current_chapter": 1,
            "user_level": "medium",
            "user_type": "beginner",
            "user_id": "test_user",
            "current_loop_conversations": []
        }
        
        quiz_state = self.quiz_agent.execute(initial_state)
        
        # í€´ì¦ˆ ìƒì„± í™•ì¸
        assert quiz_state["ui_mode"] == "quiz"
        assert quiz_state["current_stage"] == "quiz_solving"
        assert len(quiz_state["current_loop_conversations"]) > 0
        
        # ìƒì„±ëœ í€´ì¦ˆ ë°ì´í„° ì¶”ì¶œ
        quiz_data = None
        for conv in quiz_state["current_loop_conversations"]:
            if conv.get("agent") == "QuizGenerator" and "quiz_data" in conv:
                quiz_data = conv["quiz_data"]
                break
        
        assert quiz_data is not None
        assert quiz_data["question_type"] == "multiple_choice"
        
        # 2. ì‚¬ìš©ì ë‹µë³€ ì‹œë®¬ë ˆì´ì…˜
        correct_answer = quiz_data["correct_answer"]
        quiz_state["user_answer"] = correct_answer  # ì •ë‹µ ì„ íƒ
        quiz_state["hint_used"] = False
        quiz_state["response_time"] = 45
        
        # 3. í‰ê°€ ë° í”¼ë“œë°± ìƒì„±
        eval_state = self.eval_agent.execute(quiz_state)
        
        # í‰ê°€ ê²°ê³¼ í™•ì¸
        assert eval_state["ui_mode"] == "feedback"
        assert eval_state["current_stage"] == "feedback_review"
        assert "last_evaluation" in eval_state
        assert "last_feedback" in eval_state
        
        # ì •ë‹µì´ë¯€ë¡œ ê¸ì •ì  í”¼ë“œë°± í™•ì¸
        evaluation = eval_state["last_evaluation"]
        assert evaluation["is_correct"] == True
        assert evaluation["score"] > 0
        
        feedback_message = eval_state["system_message"]
        assert "âœ…" in feedback_message or "ğŸ‰" in feedback_message
    
    def test_complete_prompt_practice_flow(self):
        """ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ ì‹¤ìŠµ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # 1. í”„ë¡¬í”„íŠ¸ í€´ì¦ˆ ìƒì„±
        initial_state = {
            "user_message": "í”„ë¡¬í”„íŠ¸ ì‘ì„± ë¬¸ì œë¥¼ ë‚´ì£¼ì„¸ìš”",
            "current_chapter": 3,
            "user_level": "medium",
            "user_type": "business",
            "user_id": "test_user",
            "current_loop_conversations": []
        }
        
        quiz_state = self.quiz_agent.execute(initial_state)
        
        # í€´ì¦ˆ ìƒì„± í™•ì¸
        assert quiz_state["ui_mode"] == "quiz"
        assert "í”„ë¡¬í”„íŠ¸" in quiz_state["system_message"]
        
        # ìƒì„±ëœ í€´ì¦ˆ ë°ì´í„° ì¶”ì¶œ
        quiz_data = None
        for conv in quiz_state["current_loop_conversations"]:
            if conv.get("agent") == "QuizGenerator" and "quiz_data" in conv:
                quiz_data = conv["quiz_data"]
                break
        
        assert quiz_data is not None
        assert quiz_data["question_type"] == "prompt_practice"
        assert "scenario" in quiz_data
        assert "requirements" in quiz_data
        
        # 2. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ë‹µë³€ ì‹œë®¬ë ˆì´ì…˜
        user_prompt = """
ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ê³ ê°ì„œë¹„ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤.
ê³ ê°ì˜ ë¬¸ì˜ì‚¬í•­ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µí•´ì£¼ì„¸ìš”:

1. ë¨¼ì € ê³ ê°ì˜ ìƒí™©ì„ ê³µê°í•˜ë©° ì¸ì‚¬
2. ë¬¸ì œ ìƒí™©ì„ êµ¬ì²´ì ìœ¼ë¡œ íŒŒì•…
3. ë‹¨ê³„ë³„ í•´ê²° ë°©ì•ˆ ì œì‹œ
4. ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•œì§€ í™•ì¸

ì‘ë‹µì€ ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """.strip()
        
        quiz_state["user_answer"] = user_prompt
        quiz_state["hint_used"] = False
        quiz_state["response_time"] = 180
        
        # ChatGPT í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        chatgpt_result = self.eval_agent._simulate_chatgpt_test(user_prompt)
        quiz_state["chatgpt_test_result"] = chatgpt_result
        
        # 3. í‰ê°€ ë° í”¼ë“œë°± ìƒì„±
        eval_state = self.eval_agent.execute(quiz_state)
        
        # í‰ê°€ ê²°ê³¼ í™•ì¸
        assert eval_state["ui_mode"] == "feedback"
        assert "last_evaluation" in eval_state
        assert "last_feedback" in eval_state
        
        evaluation = eval_state["last_evaluation"]
        assert evaluation["question_type"] == "prompt_practice"
        assert "requirements_analysis" in evaluation["detailed_analysis"]
        
        feedback_message = eval_state["system_message"]
        assert "í”„ë¡¬í”„íŠ¸" in feedback_message or "ì ìˆ˜" in feedback_message
    
    def test_hint_request_and_evaluation_flow(self):
        """íŒíŠ¸ ìš”ì²­ í›„ í‰ê°€ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # 1. í€´ì¦ˆ ìƒì„±
        initial_state = {
            "user_message": "ê°ê´€ì‹ ë¬¸ì œë¥¼ ë‚´ì£¼ì„¸ìš”",
            "current_chapter": 1,
            "user_level": "medium",
            "user_type": "beginner",
            "user_id": "test_user",
            "current_loop_conversations": []
        }
        
        quiz_state = self.quiz_agent.execute(initial_state)
        
        # 2. íŒíŠ¸ ìš”ì²­
        quiz_state["quiz_attempt_count"] = 2  # 2ë²ˆì§¸ ì‹œë„ë¡œ ì„¤ì •
        hint_state = self.quiz_agent.handle_hint_request(quiz_state)
        
        # íŒíŠ¸ ì œê³µ í™•ì¸
        assert hint_state.get("hint_used") == True
        assert "ğŸ’¡" in hint_state["system_message"]
        assert "last_hint_level" in hint_state
        
        # 3. íŒíŠ¸ ì‚¬ìš© í›„ ë‹µë³€
        quiz_data = None
        for conv in hint_state["current_loop_conversations"]:
            if conv.get("agent") == "QuizGenerator" and "quiz_data" in conv:
                quiz_data = conv["quiz_data"]
                break
        
        hint_state["user_answer"] = quiz_data["correct_answer"]  # ì •ë‹µ ì„ íƒ
        hint_state["response_time"] = 90
        
        # 4. í‰ê°€ (íŒíŠ¸ ì‚¬ìš© ê³ ë ¤)
        eval_state = self.eval_agent.execute(hint_state)
        
        # íŒíŠ¸ ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ì ìˆ˜ ê°ì  í™•ì¸
        evaluation = eval_state["last_evaluation"]
        assert evaluation["is_correct"] == True
        assert evaluation["detailed_analysis"]["hint_penalty"] < 0
        
        # í”¼ë“œë°±ì— íŒíŠ¸ ì‚¬ìš© ì–¸ê¸‰ í™•ì¸ (íŒíŠ¸ ì‚¬ìš© ì‹œ ì•½ì ì— í¬í•¨ë¨)
        evaluation = eval_state["last_evaluation"]
        feedback = eval_state["last_feedback"]
        assert evaluation["detailed_analysis"]["hint_penalty"] < 0 or any("íŒíŠ¸" in weakness for weakness in evaluation.get("weaknesses", []))
    
    def test_difficulty_adjustment_flow(self):
        """ë‚œì´ë„ ì¡°ì • í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # ì‚¬ìš©ì ì„±ê³¼ ì´ë ¥ ì‹œë®¬ë ˆì´ì…˜ (ë†’ì€ ì„±ê³¼)
        high_performance = [
            {"is_correct": True, "hint_used": False, "response_time": 30, "difficulty": "medium"},
            {"is_correct": True, "hint_used": False, "response_time": 25, "difficulty": "medium"},
            {"is_correct": True, "hint_used": False, "response_time": 35, "difficulty": "medium"}
        ]
        
        # í€´ì¦ˆ ìƒì„± (ì„±ê³¼ ë°ì´í„° í¬í•¨)
        result = quiz_generation_tool(
            chapter_id=1,
            user_level="medium",
            user_type="beginner",
            quiz_type="multiple_choice",
            user_performance=high_performance,
            user_id="test_user"
        )
        
        assert result["success"] == True
        quiz_data = result["quiz_data"]
        
        # ë†’ì€ ì„±ê³¼ë¡œ ì¸í•´ ë‚œì´ë„ê°€ ìƒìŠ¹í–ˆëŠ”ì§€ í™•ì¸
        difficulty = result.get("difficulty", "medium")
        assert difficulty in ["medium", "hard"]
        
        # ë‚œì´ë„ë³„ íŒŒë¼ë¯¸í„°ê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert "difficulty_params" in quiz_data
    
    def test_learning_progress_tracking(self):
        """í•™ìŠµ ì§„ë„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        # ì—¬ëŸ¬ ë²ˆì˜ í‰ê°€ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        evaluation_results = []
        
        for i in range(5):
            question_data = {
                "question_type": "multiple_choice",
                "options": ["A", "B", "C", "D"],
                "correct_answer": 1,
                "difficulty": "medium"
            }
            
            # ì ì§„ì ìœ¼ë¡œ í–¥ìƒë˜ëŠ” ì„±ê³¼ ì‹œë®¬ë ˆì´ì…˜
            user_answer = 1 if i >= 2 else 0  # ì²˜ìŒ 2ê°œëŠ” ì˜¤ë‹µ, ë‚˜ë¨¸ì§€ëŠ” ì •ë‹µ
            hint_used = i < 3  # ì²˜ìŒ 3ê°œëŠ” íŒíŠ¸ ì‚¬ìš©
            
            eval_result = answer_evaluation_tool(
                question_data=question_data,
                user_answer=user_answer,
                hint_used=hint_used,
                response_time=60 - i * 10  # ì ì  ë¹¨ë¼ì§
            )
            
            if eval_result["success"]:
                evaluation_results.append(eval_result["evaluation_result"])
        
        # í•™ìŠµ ì§„ë„ ê³„ì‚°
        from tools.evaluation.answer_eval_tool import calculate_learning_progress
        
        progress_result = calculate_learning_progress(
            user_id="test_user",
            chapter_id=1,
            evaluation_results=evaluation_results
        )
        
        assert progress_result["success"] == True
        progress_data = progress_result["progress_data"]
        
        # í–¥ìƒ ì¶”ì„¸ í™•ì¸
        assert progress_data["accuracy_trend"] == "improving"
        assert progress_data["total_attempts"] == 5
        assert progress_data["average_score"] > 0
    
    def test_error_handling_integration(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸"""
        # 1. ì˜ëª»ëœ í€´ì¦ˆ ë°ì´í„°ë¡œ í‰ê°€ ì‹œë„
        invalid_state = {
            "user_id": "test_user",
            "user_answer": 1,
            "current_loop_conversations": [
                {
                    "agent": "QuizGenerator",
                    "quiz_data": {
                        "question_type": "invalid_type",  # ì˜ëª»ëœ íƒ€ì…
                        "options": ["A", "B"],
                        "correct_answer": 0
                    }
                }
            ]
        }
        
        eval_state = self.eval_agent.execute(invalid_state)
        
        # ì˜¤ë¥˜ ì²˜ë¦¬ í™•ì¸
        assert "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in eval_state["system_message"]
        assert eval_state["ui_mode"] == "chat"
        
        # 2. í€´ì¦ˆ ë°ì´í„°ê°€ ì—†ëŠ” ìƒíƒœì—ì„œ í‰ê°€ ì‹œë„
        empty_state = {
            "user_id": "test_user",
            "current_loop_conversations": []
        }
        
        eval_state = self.eval_agent.execute(empty_state)
        
        assert "í‰ê°€í•  ë¬¸ì œë‚˜ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in eval_state["system_message"]
    
    def test_ui_mode_transitions(self):
        """UI ëª¨ë“œ ì „í™˜ í…ŒìŠ¤íŠ¸"""
        # ì´ˆê¸° ìƒíƒœ (chat ëª¨ë“œ)
        state = {
            "user_message": "ë¬¸ì œë¥¼ ë‚´ì£¼ì„¸ìš”",
            "current_chapter": 1,
            "user_level": "medium",
            "user_type": "beginner",
            "user_id": "test_user",
            "ui_mode": "chat",
            "current_loop_conversations": []
        }
        
        # 1. chat -> quiz ëª¨ë“œ ì „í™˜
        quiz_state = self.quiz_agent.execute(state)
        assert quiz_state["ui_mode"] == "quiz"
        assert quiz_state["current_stage"] == "quiz_solving"
        
        # 2. ë‹µë³€ ì¶”ê°€
        quiz_data = quiz_state["current_loop_conversations"][0]["quiz_data"]
        quiz_state["user_answer"] = quiz_data["correct_answer"]
        
        # 3. quiz -> feedback ëª¨ë“œ ì „í™˜
        eval_state = self.eval_agent.execute(quiz_state)
        assert eval_state["ui_mode"] == "feedback"
        assert eval_state["current_stage"] == "feedback_review"
        
        # UI ìš”ì†Œ í™•ì¸
        assert "ui_elements" in eval_state
        ui_elements = eval_state["ui_elements"]
        assert ui_elements.get("type") == "feedback_display"
        assert "score_display" in ui_elements
        assert "show_next_button" in ui_elements


class TestToolsIntegration:
    """ë„êµ¬ë“¤ ê°„ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def test_quiz_to_evaluation_tool_chain(self):
        """í€´ì¦ˆ ìƒì„± -> ë‹µë³€ í‰ê°€ ë„êµ¬ ì²´ì¸ í…ŒìŠ¤íŠ¸"""
        # 1. í€´ì¦ˆ ìƒì„±
        quiz_result = quiz_generation_tool(
            chapter_id=1,
            user_level="medium",
            user_type="beginner",
            quiz_type="multiple_choice"
        )
        
        assert quiz_result["success"] == True
        quiz_data = quiz_result["quiz_data"]
        
        # 2. ë‹µë³€ í‰ê°€
        eval_result = answer_evaluation_tool(
            question_data=quiz_data,
            user_answer=quiz_data["correct_answer"],
            hint_used=False,
            response_time=45
        )
        
        assert eval_result["success"] == True
        evaluation = eval_result["evaluation_result"]
        
        # 3. í”¼ë“œë°± ìƒì„±
        feedback_result = feedback_generation_tool(
            evaluation_result=evaluation,
            question_data=quiz_data,
            user_profile={"user_level": "medium", "user_type": "beginner"},
            learning_context={"current_chapter": 1, "recent_performance": []}
        )
        
        assert feedback_result["success"] == True
        feedback_data = feedback_result["feedback_data"]
        
        # ì „ì²´ ì²´ì¸ ê²°ê³¼ í™•ì¸
        assert evaluation["is_correct"] == True
        assert "main_feedback" in feedback_data
        assert "encouragement" in feedback_data
        assert "improvement_tips" in feedback_data
    
    def test_hint_to_evaluation_integration(self):
        """íŒíŠ¸ -> í‰ê°€ í†µí•© í…ŒìŠ¤íŠ¸"""
        from tools.content.hint_tool import hint_generation_tool
        
        # 1. ë¬¸ì œ ë°ì´í„° ì¤€ë¹„
        question_data = {
            "question_id": "test_q1",
            "question_type": "multiple_choice",
            "difficulty": "medium",
            "question_text": "AIê°€ ë¬´ì—‡ì˜ ì¤„ì„ë§ì¸ê°€ìš”?",
            "options": ["Artificial Intelligence", "Advanced Internet", "Automatic Information", "Applied Integration"],
            "correct_answer": 0
        }
        
        # 2. íŒíŠ¸ ìƒì„±
        hint_result = hint_generation_tool(
            question_data=question_data,
            hint_level=1,
            user_level="medium",
            attempt_count=2
        )
        
        assert hint_result["success"] == True
        
        # 3. íŒíŠ¸ ì‚¬ìš© í›„ ë‹µë³€ í‰ê°€
        eval_result = answer_evaluation_tool(
            question_data=question_data,
            user_answer=0,  # ì •ë‹µ
            hint_used=True,  # íŒíŠ¸ ì‚¬ìš©
            response_time=90
        )
        
        assert eval_result["success"] == True
        evaluation = eval_result["evaluation_result"]
        
        # íŒíŠ¸ ì‚¬ìš©ì´ í‰ê°€ì— ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert evaluation["is_correct"] == True
        assert evaluation["detailed_analysis"]["hint_penalty"] < 0
        assert evaluation["score"] < 100  # íŒíŠ¸ í˜ë„í‹°ë¡œ ì¸í•œ ê°ì 


if __name__ == "__main__":
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì„ ìœ„í•œ ì½”ë“œ
    pytest.main([__file__, "-v"])